import numpy as np
import os
import torch
import logging

from core.task_generator import TaskGenerator
from drl.agents.ddqn_agent import DDQNAgent
from drl.envs.data_loader import DataLoader
from drl.envs.environment import Environment
from drl.trainer.ddqn_trainer import DDQNTrainer
from config.config import DEVICE
from config.drl_config import ddqn_config, epoch_size
from ray.tune.registry import register_env

logging.basicConfig(
    level=logging.INFO,  # c√≥ th·ªÉ ƒë·ªïi th√†nh INFO khi mu·ªën gi·∫£m log, DEBUG ƒë·ªÉ hi·ªÉn th·ªã to√†n b·ªô log
    format="%(asctime)s [%(levelname)s] [%(name)s] %(message)s",
    datefmt="%H:%M:%S",
    handlers=[
        logging.FileHandler("./logs/app.log", mode="a", encoding="utf-8"),
        logging.StreamHandler(),  # In ra m√†n h√¨nh
    ],
)

logger = logging.getLogger(__name__)

if DEVICE != "cpu":
    device = torch.device("cuda:" + str(DEVICE) if torch.cuda.is_available() else "cpu")
else:
    device = torch.device("cpu")
if device == "cpu":
    print("cannot train with cpu")
    exit(0)
else:
    print("cuda: ", device)


def create_agent(
    state_dim,
    action_dim,
    agent_idx=0,
    load_model=False,
    checkpoint_idx=0,
):
    """
    T·∫°o m·ªôt agent (DDQN) v·ªõi c√°c tham s·ªë ƒë·ªãnh nghƒ©a.

    Args:
        state_dim (int): S·ªë chi·ªÅu tr·∫°ng th√°i ƒë·∫ßu v√†o.
        action_dim (int): S·ªë chi·ªÅu h√†nh ƒë·ªông.
        agent_idx (int): Ch·ªâ s·ªë c·ªßa agent (d√πng cho checkpoint).
        load_model (bool): N·∫øu True, load model t·ª´ file.
        checkpoint_idx (int): S·ªë th·ª© t·ª± checkpoint n·∫øu mu·ªën load nhi·ªÅu version.

    Returns:
        agent: M·ªôt object agent (DDQNAgent) ƒë√£ kh·ªüi t·∫°o.
    """

    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n checkpoint
    if checkpoint_idx == 0:
        checkpoint_path = f"./checkpoints/agent_{agent_idx}.pth"
    else:
        checkpoint_path = f"./checkpoints/agent_{agent_idx}_{checkpoint_idx}.pth"

    # Kh·ªüi t·∫°o Agent
    agent = DDQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        model_path=checkpoint_path,
        load_pretrained=load_model,
    )

    return agent


def create_trainer(
    environment,
    agent_list,
    save_directory,
    update_interval=500,
    max_episode_length=100,
    score_window_size=100,
    use_thread=True,
    detach_thread=True,
):
    """
    T·∫°o v√† kh·ªüi t·∫°o trainer ƒë·ªÉ hu·∫•n luy·ªán c√°c agent trong m√¥i tr∆∞·ªùng.

    Args:
        environment: M√¥i tr∆∞·ªùng m√¥ ph·ªèng ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° agent.
        agent_list: Danh s√°ch c√°c agent c·∫ßn hu·∫•n luy·ªán.
        save_directory: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u m√¥ h√¨nh, checkpoint v√† log.
        update_interval: S·ªë b∆∞·ªõc gi·ªØa c√°c l·∫ßn c·∫≠p nh·∫≠t m·∫°ng m·ª•c ti√™u.
        max_episode_length: S·ªë b∆∞·ªõc t·ªëi ƒëa trong m·ªói episode.
        score_window_size: K√≠ch th∆∞·ªõc c·ª≠a s·ªï ƒë·ªÉ t√≠nh ƒëi·ªÉm trung b√¨nh.
        use_thread: C√≥ s·ª≠ d·ª•ng ƒëa lu·ªìng trong hu·∫•n luy·ªán hay kh√¥ng.
        detach_thread: C√≥ t√°ch lu·ªìng kh·ªèi ti·∫øn tr√¨nh ch√≠nh hay kh√¥ng.
        trainer_type: Lo·∫°i trainer (m·∫∑c ƒë·ªãnh l√† "DDQNTrainer").

    Returns:
        Trainer: ƒê·ªëi t∆∞·ª£ng trainer ƒë∆∞·ª£c kh·ªüi t·∫°o s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán.
    """

    # ƒê·∫£m b·∫£o th∆∞ m·ª•c l∆∞u tr·ªØ t·ªìn t·∫°i
    os.makedirs(save_directory, exist_ok=True)

    # Kh·ªüi t·∫°o trainer v·ªõi c√°c tham s·ªë c·∫•u h√¨nh
    trainer = DDQNTrainer(
        env=environment,
        agents=agent_list,
        score_window_size=score_window_size,
        max_episode_length=max_episode_length,
        update_frequency=update_interval,
        save_dir=save_directory,
        use_thread=use_thread,
        detach_thread=detach_thread,
        train_start_factor=2,
    )

    # Kh·ªüi t·∫°o b·ªô ƒë·∫øm
    trainer.current_step = 0
    trainer.current_episode = 0

    return trainer


def train_agents(
    env, trainer, max_episodes=100000, target_score=100000, score_window=100
):
    """
    Th·ª±c hi·ªán qu√° tr√¨nh hu·∫•n luy·ªán c√°c agent trong m√¥i tr∆∞·ªùng.

    Args:
        env: M√¥i tr∆∞·ªùng RL (multi-agent environment).
        trainer: ƒê·ªëi t∆∞·ª£ng trainer (MAPPOTrainer ho·∫∑c DDQNTrainer) qu·∫£n l√Ω qu√° tr√¨nh hu·∫•n luy·ªán.
        max_episodes: S·ªë l∆∞·ª£ng t·ªëi ƒëa episode hu·∫•n luy·ªán.
        target_score: M·ª©c ƒëi·ªÉm trung b√¨nh t·ªëi thi·ªÉu ƒë·ªÉ coi l√† ƒë√£ "ho√†n th√†nh" m√¥i tr∆∞·ªùng.
        score_window: S·ªë episode g·∫ßn nh·∫•t ƒë·ªÉ t√≠nh ƒëi·ªÉm trung b√¨nh.
    """
    logger.info("===== B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN AGENTS =====")
    logger.info(f"Max episodes: {max_episodes}")
    logger.info(f"Target score: {target_score}")
    logger.info(f"Score window: {score_window}")
    logger.info(f"S·ªë agents: {len(trainer.agents)}")
    logger.info(f"M√¥i tr∆∞·ªùng: {env.__class__.__name__}")
    logger.info(f"Trainer: {trainer.__class__.__name__}")

    for episode_idx in range(1, max_episodes + 1):
        try:
            logger.debug(f"[Episode {episode_idx}] B·∫Øt ƒë·∫ßu episode...")
            # Th·ª±c hi·ªán 1 b∆∞·ªõc hu·∫•n luy·ªán (episode)
            trainer.run_episode_step()

            # In tr·∫°ng th√°i hu·∫•n luy·ªán ƒë·ªãnh k·ª≥
            if episode_idx % 100 == 0:
                trainer.print_status()

            # T√≠nh ƒëi·ªÉm trung b√¨nh c·ªßa c√°c episode g·∫ßn nh·∫•t
            recent_scores = trainer.score_history[-score_window:]
            logger.debug(
                f"[Episode {episode_idx}] Score history length: {len(trainer.score_history)}"
            )
            mean_reward = np.max(recent_scores, axis=1).mean()
            logger.info(
                f"Episode {episode_idx} - Mean reward (last {score_window} episodes): {mean_reward:.2f}"
            )

            logger.debug(
                f"[Episode {episode_idx}] Mean reward computed from max rewards per episode."
            )

            # L∆∞u model v√† plot ƒë·ªãnh k·ª≥
            if episode_idx % epoch_size == 0:
                logger.debug(f"[Episode {episode_idx}] L∆∞u model v√† plot ƒë·ªãnh k·ª≥.")
                trainer.save_models()
                trainer.print_status()
                trainer.df_scores()
            elif episode_idx % score_window == 0:
                logger.debug(
                    f"[Episode {episode_idx}] C·∫≠p nh·∫≠t df_scores() theo score_window."
                )
                trainer.print_status()
                trainer.df_scores()

            # D·ª´ng hu·∫•n luy·ªán n·∫øu ƒë·∫°t target_score ho·∫∑c h·∫øt max_episodes
            if mean_reward >= target_score:
                logger.info(
                    f"‚õ≥ Target ƒë·∫°t ƒë∆∞·ª£c! Mean reward = {mean_reward:.2f} >= {target_score}"
                )
                logger.debug("B·∫Øt ƒë·∫ßu l∆∞u model cu·ªëi c√πng tr∆∞·ªõc khi tho√°t.")
                trainer.save_models()
                trainer.print_status()
                trainer.df_scores()
                logger.debug("ƒê√≥ng m√¥i tr∆∞·ªùng.")
                env.close()
                break

            if episode_idx == max_episodes:
                logger.info("üõë ƒê√£ ƒë·∫°t max_episodes, d·ª´ng hu·∫•n luy·ªán.")
                trainer.save_models()
                trainer.print_status()
                trainer.df_scores()
                logger.debug("ƒê√≥ng m√¥i tr∆∞·ªùng.")
                env.close()
                break
        except Exception as e:
            logger.error(f"[ERROR] Running error: {e}")

# ddqn
def run_ddqn_training(**kwargs):
    """
    Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán c√°c agent s·ª≠ d·ª•ng thu·∫≠t to√°n DDQN trong m√¥i tr∆∞·ªùng ITS.

    Tham s·ªë:
        verbose (bool, optional): N·∫øu True, in log chi ti·∫øt trong qu√° tr√¨nh hu·∫•n luy·ªán. M·∫∑c ƒë·ªãnh l√† False.

    Quy tr√¨nh:
        1. T·∫£i d·ªØ li·ªáu b·∫£n ƒë·ªì v√† th√¥ng tin m√¥i tr∆∞·ªùng.
        2. T·∫°o b·ªô sinh t√°c v·ª• (TaskGenerator).
        3. Ghi c·∫•u h√¨nh m√¥i tr∆∞·ªùng.
        4. ƒêƒÉng k√Ω v√† kh·ªüi t·∫°o m√¥i tr∆∞·ªùng hu·∫•n luy·ªán.
        5. X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng agent, k√≠ch th∆∞·ªõc tr·∫°ng th√°i v√† h√†nh ƒë·ªông.
        6. T·∫°o danh s√°ch c√°c agent DDQN.
        7. C·∫•u h√¨nh th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£.
        8. Kh·ªüi t·∫°o Trainer v√† b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán.
    """
    verbose = kwargs.get("verbose", True)

    # --- 1. Load environment and map information ---
    data_loader = DataLoader()
    graph, map_info = data_loader.get_graph_and_map()

    # --- 2. Create task generator ---
    task_gen = TaskGenerator(1, map_info)

    # --- 3. Write environment config ---
    env_config = DataLoader.generate_config_not_from_file(mission_generator=task_gen)

    # --- 4. Register and create environment ---
    register_env(
        "env",
        lambda config: Environment(
            env_data=config, verbose=verbose, map_obj=map_info, task_generator=task_gen
        ),
    )

    env = Environment(
        env_data=env_config, verbose=verbose, map_obj=map_info, task_generator=task_gen
    )

    # --- 5. Extract environment dimensions ---
    num_agents = env_config["num_vehicles"]
    state_dim = np.prod(env.observation_space.shape)
    action_dim = env.action_space.shape[0]

    logger.info(
        f"====== num_agents: {num_agents}, state_dim: {state_dim}, action_dim: {action_dim}"
    )

    # --- 6. Initialize DDQN agents ---
    agents = []
    for i in range(num_agents):
        agent = create_agent(
            state_dim, action_dim, agent_idx=i, load_model=False, checkpoint_idx=0
        )
        agents.append(agent)

    # --- 7. Prepare save directory ---
    save_dir = os.path.join(
        os.getcwd(),
        "saved_files_global_combine_decay_{decay}_lr_{lr}_batch_{bs}_reward_{rw}_combine_{cb}_more".format(
            decay=ddqn_config["epsilon_decay"],
            lr=ddqn_config["learning_rate"],
            bs=ddqn_config["batch_size"],
            rw=ddqn_config["modify_reward"],
            cb=ddqn_config["combine"],
        ),
    )

    # --- 8. Create trainer ---
    trainer = create_trainer(
        env,
        agents,
        save_dir,
        use_thread=env_config["apply_thread"],
        detach_thread=env_config["apply_detach"],
        score_window_size=env_config["score_window_size"],
        max_episode_length=env_config["max_missions_per_vehicle"] * env_config["num_vehicles"],
        update_interval=ddqn_config["batch_size"] / 4,
    )

    # --- 9. Train agents ---
    train_agents(env, trainer, score_window=env_config["score_window_size"])


if __name__ == "__main__":
    run_ddqn_training()
